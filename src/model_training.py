from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
import lightgbm as lgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
import base64
import ember
import random
import numpy as np

# Load the ember dataset
X, y= ember.read_vectorized_features("/ember/ember2018/", subset='train')
print("Completed reading data")


# Function to obfuscate strings using base64 encoding
def obfuscate_strings(data):
    for i, file_data in enumerate(data):
        if 'string_features' in file_data:
            strings = file_data['string_features']
            obfuscated_strings = [base64.b64encode(s.encode()).decode() for s in strings]
            data[i]['string_features'] = obfuscated_strings
    return data

# Simulate opcode control flow obfuscation
def obfuscate_opcodes(data):
    for i, file_data in enumerate(data):
        if 'byte_histogram' in file_data:  # Assuming opcodes are embedded here
            opcodes = file_data['byte_histogram']
            random.shuffle(opcodes)  # Shuffle opcodes to simulate obfuscation
            data[i]['byte_histogram'] = opcodes
    return data

# Opcode substitution to obscure instructions
def substitute_opcodes(data, substitution_map):
    for i, file_data in enumerate(data):
        if 'byte_histogram' in file_data:
            opcodes = file_data['byte_histogram']
            substituted_opcodes = [substitution_map.get(op, op) for op in opcodes]
            data[i]['byte_histogram'] = substituted_opcodes
    return data

# Apply string obfuscation
X = obfuscate_strings(X)

# Apply opcode obfuscation
X = obfuscate_opcodes(X)
# X_test_obfuscated = obfuscate_opcodes(X_test_obfuscated)

# Example substitution map: mapping certain common opcodes to others
substitution_map = {0x90: 0x91, 0xC3: 0xC2}  # Example: replace NOP (0x90) with XCHG (0x91)

# Apply opcode substitution
X = substitute_opcodes(X, substitution_map)

# Split the dataset into training and testing
X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=50000,test_size=10000)


# Feature Scaling
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Handle missing data (if any)
imputer = SimpleImputer(strategy='median')
X_train_scaled = imputer.fit_transform(X_train_scaled)
X_test_scaled = imputer.transform(X_test_scaled)


train_labeled_rows = (y_train != -1)
test_labeled_rows = (y_test != -1)

X_train = X_train_scaled[train_labeled_rows]
y_train = y_train[train_labeled_rows]

X_test = X_test_scaled[test_labeled_rows]
y_test = y_test[test_labeled_rows]


lgb_params = {
            "boosting": "gbdt",
            "objective": "binary",
            "num_iterations": 100,
            "learning_rate": 0.05,
            "num_leaves": 2048,
            "max_depth": 15,
            "min_data_in_leaf": 50,
            "feature_fraction": 0.5
        }

# Train
# lgbm_dataset = lgb.Dataset(X_train, y_train)
# lgbm_model = lgb.train(lgb_params, lgbm_dataset)

# Random Forest Model
rf_model = RandomForestClassifier(random_state=42)
rf_model.fit(X_train, y_train)

# XGBoost Model
xgb_model = XGBClassifier(random_state=42)
xgb_model.fit(X_train, y_train)


# Random Forest Evaluation
# lgbm_preds = lgbm_model.predict(X_test)
# lgbm_accuracy = accuracy_score(y_test, lgbm_preds)
# lgbm_precision = precision_score(y_test, lgbm_preds)
# lgbm_recall = recall_score(y_test, lgbm_preds)
# lgbm_f1 = f1_score(y_test, lgbm_preds)

# Random Forest Evaluation
rf_preds = rf_model.predict(X_test)
rf_accuracy = accuracy_score(y_test, rf_preds)
rf_precision = precision_score(y_test, rf_preds, average='macro')
rf_recall = recall_score(y_test, rf_preds, average='macro')
rf_f1 = f1_score(y_test, rf_preds, average='macro')

# XGBoost Evaluation
xgb_preds = xgb_model.predict(X_test)
xgb_accuracy = accuracy_score(y_test, xgb_preds)
xgb_precision = precision_score(y_test, xgb_preds, average='macro')
xgb_recall = recall_score(y_test, xgb_preds, average='macro')
xgb_f1 = f1_score(y_test, xgb_preds, average='macro')

# DNN Evaluation
# dnn_preds = dnn_model.predict(X_test_scaled)
# dnn_preds = (dnn_preds > 0.5).astype(int)
# dnn_accuracy = accuracy_score(y_test, dnn_preds)
# dnn_precision = precision_score(y_test, dnn_preds)
# dnn_recall = recall_score(y_test, dnn_preds)
# dnn_f1 = f1_score(y_test, dnn_preds)

# Print Results
# print("LGBM - Accuracy: {:.2f}, Precision: {:.2f}, Recall: {:.2f}, F1-Score: {:.2f}".format(lgbm_accuracy, lgbm_precision, lgbm_recall, lgbm_f1))
print("Random Forest - Accuracy: {:.2f}, Precision: {:.2f}, Recall: {:.2f}, F1-Score: {:.2f}".format(rf_accuracy, rf_precision, rf_recall, rf_f1))
print("XGBoost - Accuracy: {:.2f}, Precision: {:.2f}, Recall: {:.2f}, F1-Score: {:.2f}".format(xgb_accuracy, xgb_precision, xgb_recall, xgb_f1))

# Deep Neural Network (DNN) Model
# dnn_model = Sequential()
# dnn_model.add(Dense(128, input_dim=X_train_scaled.shape[1], activation='relu'))
# dnn_model.add(Dense(64, activation='relu'))
# dnn_model.add(Dense(1, activation='sigmoid'))
# dnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
# dnn_model.fit(X_train_scaled, y_train, epochs=10, batch_size=32)

# params = {
#             "boosting": "gbdt",
#             "objective": "binary",
#             "num_iterations": 1000,
#             "learning_rate": 0.05,
#             "num_leaves": 2048,
#             "max_depth": 15,
#             "min_data_in_leaf": 50,
#             "feature_fraction": 0.5
#         }

# # Evaluate the model
# y_pred_train = clf.predict(X_train)
# y_pred_test = clf.predict(X_test)
# print("Accuracy on obfuscated train dataset:", accuracy_score(y_train, y_pred_train))
# print("Accuracy on obfuscated test dataset:", accuracy_score(y_test, y_pred_test))
